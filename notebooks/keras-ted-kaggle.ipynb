{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talks keyword labeling with pre-trained word embeddings\n",
    "\n",
    "In this notebook, we'll use pre-trained [GloVe word embeddings](http://nlp.stanford.edu/projects/glove/) for keyword labeling using Keras (version $\\ge$ 2 is required). This notebook is largely based on the blog post [Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) by FranÃ§ois Chollet.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports. Keras tells us which backend (Theano, Tensorflow, CNTK) it will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using TensorFlow as the backend, we can use TensorBoard to visualize our progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.backend() == \"tensorflow\":\n",
    "    import tensorflow as tf\n",
    "    print('TensorFlow version:', tf.__version__)\n",
    "    from keras.callbacks import TensorBoard\n",
    "    import os, datetime\n",
    "    logdir = os.path.join(os.getcwd(), \"logs\",\n",
    "                     \"ted-\"+datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    print('TensorBoard log directory:', logdir)\n",
    "    os.makedirs(logdir)\n",
    "    callbacks = [TensorBoard(log_dir=logdir)]\n",
    "else:\n",
    "    callbacks =  None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word embeddings\n",
    "\n",
    "Let's begin by loading a datafile containing pre-trained word embeddings from [Pouta Object Storage](https://research.csc.fi/pouta-object-storage).  The datafile contains 100-dimensional embeddings for 400,000 English words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://object.pouta.csc.fi/swift/v1/AUTH_dac/mldata/glove6b100dtxt.zip\n",
    "!unzip -u glove6b100dtxt.zip\n",
    "GLOVE_DIR = \".\"\n",
    "\n",
    "#GLOVE_DIR = \"/home/cloud-user/machine-learning-scripts/notebooks\"\n",
    "#GLOVE_DIR = \"/home/cloud-user/glove.6B\"\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = len(coefs)\n",
    "print('Found %d word vectors of dimensionality %d.' % (len(embeddings_index), embedding_dim))\n",
    "\n",
    "print('Examples of embeddings:')\n",
    "for w in ['some', 'random', 'words']:\n",
    "    print(w, embeddings_index[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TED Talks data set\n",
    "\n",
    "Next we'll load the TED Talks data set (Kaggle [TED Talks](https://www.kaggle.com/rounakbanik/ted-talks), 2017 edition).  The data is stored in two CSV files, so we load both of them and merge them into a single DataFrame. \n",
    "\n",
    "The merged dataset contains transcripts and metadata of 2467 TED talks. Each talk is also annotated with a set of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://object.pouta.csc.fi/swift/v1/AUTH_dac/mldata/ted-talks.zip\n",
    "!unzip -u ted-talks.zip\n",
    "TEXT_DATA_DIR = \".\"\n",
    "\n",
    "#TEXT_DATA_DIR = \"/home/cloud-user/ted/kaggle-ted-talks\"\n",
    "\n",
    "df1 = pd.read_csv(TEXT_DATA_DIR+'/ted_main.csv')\n",
    "df2 = pd.read_csv(TEXT_DATA_DIR+'/transcripts.csv')\n",
    "df = pd.merge(left=df1, right=df2, how='inner', left_on='url', right_on='url')\n",
    "\n",
    "print(len(df), 'talks')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual data\n",
    "\n",
    "There are two potential columns to be used as the input text source: `transcript` and `description`. The former is the full transcript of the talk, whereas the latter is a shorter abstract of the contents of the talk. \n",
    "\n",
    "Let's inspect the distributions of the lengths of these columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_trans, len_descr = np.empty(len(df)), np.empty(len(df))\n",
    "for i, row in df.iterrows():\n",
    "   len_trans[i]=len(row['transcript'])\n",
    "   len_descr[i]=len(row['description'])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title('Length of descriptions, mean: %.2f' % np.mean(len_descr))\n",
    "plt.xlabel('words')\n",
    "plt.hist(len_descr, 'auto')\n",
    "plt.subplot(122)\n",
    "plt.title('Length of transcripts, mean: %.2f' % np.mean(len_trans))\n",
    "plt.xlabel('words')\n",
    "plt.hist(len_trans, 'auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we decide to use either the `transcipt` or the `description` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = \"transcript\"\n",
    "#texttype = \"description\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "\n",
    "Let's start by converting the string-type lists of tags to Python lists.  Then, we take a look at a histogram of number of tags attached to talks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['taglist']=df['tags'].apply(lambda x: ast.literal_eval(x))\n",
    "df.head()\n",
    "\n",
    "l = np.empty(len(df))\n",
    "for i, v in df['taglist'].iteritems():\n",
    "    l[i]=len(v)\n",
    "plt.figure()\n",
    "plt.title('Number of tags, mean: %.2f' % np.mean(l))\n",
    "plt.xlabel('labels')\n",
    "plt.hist(l,np.arange(40)+1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `NLABELS` most frequent tags as keyword labels we wish to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLABELS=100\n",
    "\n",
    "ntags = dict()\n",
    "for tl in df['taglist']:\n",
    "    for t in tl:\n",
    "        if t in ntags:\n",
    "            ntags[t] += 1\n",
    "        else:\n",
    "            ntags[t] = 1\n",
    "\n",
    "ntagslist_sorted = sorted(ntags, key=ntags.get, reverse=True)\n",
    "print('Total of', len(ntagslist_sorted), 'tags found. Showing', NLABELS, 'most common tags:')\n",
    "for i, t in enumerate(ntagslist_sorted[:NLABELS]):\n",
    "    print(i, t, ntags[t])\n",
    "\n",
    "def tags_to_indices(x):\n",
    "    ilist = []\n",
    "    for t in x:\n",
    "        ilist.append(ntagslist_sorted.index(t))\n",
    "    return ilist\n",
    "\n",
    "df['tagidxlist'] = df['taglist'].apply(tags_to_indices)\n",
    "\n",
    "def indices_to_labels(x):\n",
    "    labels = np.zeros(NLABELS)\n",
    "    for i in x:\n",
    "        if i < NLABELS:\n",
    "            labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "df['labels'] = df['tagidxlist'].apply(indices_to_labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce input and label tensors\n",
    "\n",
    "We vectorize the text samples and labels into a 2D integer tensors. `MAX_NUM_WORDS` is the number of different words to use as tokens, selected based on word frequency. `MAX_SEQUENCE_LENGTH` is the fixed sequence length obtained by truncating or padding the original sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000 \n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts([x for x in df[texttype]])\n",
    "sequences = tokenizer.texts_to_sequences([x for x in df[texttype]])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.asarray([x for x in df['labels']])\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into a training set and a validation set.  We use a fraction of the data specified by `VALIDATION_DATA` for validation.  Note that we do not use a separate test set in this notebook, due to the small size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "print('Shape of training data tensor:', x_train.shape)\n",
    "print('Shape of training label tensor:', y_train.shape)\n",
    "print('Shape of validation data tensor:', x_val.shape)\n",
    "print('Shape of validation label tensor:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the embedding matrix by retrieving the corresponding word embedding for each token in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Shape of embedding matrix:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D CNN\n",
    "\n",
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(NLABELS, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 20\n",
    "batch_size=16\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "To further analyze the results, we can produce the actual predictions for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected threshold controls the number of label predictions we'll make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "avg_n_gt, avg_n_pred = 0, 0\n",
    "for t in range(len(y_val)):\n",
    "    avg_n_gt += len(np.where(y_val[t]>0.5)[0])\n",
    "    avg_n_pred += len(np.where(predictions[t]>threshold)[0])\n",
    "avg_n_gt /= len(y_val)\n",
    "avg_n_pred /= len(y_val)\n",
    "print('Average number of ground-truth labels per talk: %.2f' % avg_n_gt)\n",
    "print('Average number of predicted labels per talk: %.2f' % avg_n_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correct and predicted labels for some talks in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_talks_to_show = 20\n",
    "\n",
    "for t in range(nb_talks_to_show):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_val[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has some applicable performance [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) we can try: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.precision_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('Recall: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.recall_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('F1 score: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.f1_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_val.flatten(), predictions.flatten())\n",
    "print('Average precision: {0:.3f}'.format(average_precision))\n",
    "print('Coverage: {0:.3f}'\n",
    "      .format(metrics.coverage_error(y_val, predictions)))\n",
    "print('LRAP: {0:.3f}'\n",
    "      .format(metrics.label_ranking_average_precision_score(y_val, predictions)))\n",
    "\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_val.flatten(), predictions.flatten())\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-recall curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(CuDNNLSTM(128))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(NLABELS, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 20\n",
    "batch_size=16\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "avg_n_gt, avg_n_pred = 0, 0\n",
    "for t in range(len(y_val)):\n",
    "    avg_n_gt += len(np.where(y_val[t]>0.5)[0])\n",
    "    avg_n_pred += len(np.where(predictions[t]>threshold)[0])\n",
    "avg_n_gt /= len(y_val)\n",
    "avg_n_pred /= len(y_val)\n",
    "print('Average number of ground-truth labels per talk: %.2f' % avg_n_gt)\n",
    "print('Average number of predicted labels per talk: %.2f' % avg_n_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_talks_to_show = 20\n",
    "\n",
    "for t in range(nb_talks_to_show):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_val[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.precision_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('Recall: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.recall_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('F1 score: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.f1_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_val.flatten(), predictions.flatten())\n",
    "print('Average precision: {0:.3f}'.format(average_precision))\n",
    "print('Coverage: {0:.3f}'\n",
    "      .format(metrics.coverage_error(y_val, predictions)))\n",
    "print('LRAP: {0:.3f}'\n",
    "      .format(metrics.label_ranking_average_precision_score(y_val, predictions)))\n",
    "\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_val.flatten(), predictions.flatten())\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-recall curve');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

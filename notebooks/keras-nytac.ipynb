{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Times Annotated Corpus keyword labeling with pre-trained word embeddings\n",
    "\n",
    "In this notebook, we'll use pre-trained [GloVe word embeddings](http://nlp.stanford.edu/projects/glove/) for keyword labeling using Keras (version $\\ge$ 2 is required). This notebook is largely based on the blog post [Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) by FranÃ§ois Chollet.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports. Keras tells us which backend (Theano, Tensorflow, CNTK) it will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using TensorFlow as the backend, we can use TensorBoard to visualize our progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.backend() == \"tensorflow\":\n",
    "    import tensorflow as tf\n",
    "    print('TensorFlow version:', tf.__version__)\n",
    "    from keras.callbacks import TensorBoard\n",
    "    import os, datetime\n",
    "    logdir = os.path.join(os.getcwd(), \"logs\",\n",
    "                     \"ted-\"+datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    print('TensorBoard log directory:', logdir)\n",
    "    os.makedirs(logdir)\n",
    "    callbacks = [TensorBoard(log_dir=logdir)]\n",
    "else:\n",
    "    callbacks =  None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word embeddings\n",
    "\n",
    "Let's begin by loading a datafile containing pre-trained word embeddings.  The datafile contains 100-dimensional embeddings for 400,000 English words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget --content-disposition -nc https://kannu.csc.fi/s/rrCNCRdJf9LZSCE/download\n",
    "GLOVE_DIR = \"/home/cloud-user/machine-learning-scripts/notebooks\"\n",
    "\n",
    "#GLOVE_DIR = \"/home/cloud-user/glove.6B\"\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = len(coefs)\n",
    "print('Found %d word vectors of dimensionality %d.' % (len(embeddings_index), embedding_dim))\n",
    "\n",
    "print('Examples of embeddings:')\n",
    "for w in ['some', 'random', 'words']:\n",
    "    print(w, embeddings_index[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Times Annotated Corpus\n",
    "\n",
    "Next we'll load the [New York Times Annotated Corpus](https://catalog.ldc.upenn.edu/ldc2008t19).  The data is originally in stored in article-wise XML files, but we load a portion of the data from a preprocessed HDF5 file instead. \n",
    "\n",
    "The preprocessed dataset contains articles of year 1987. Each article is annotated with a set of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = \"/home/cloud-user/nytac\"\n",
    "\n",
    "store = pd.HDFStore(TEXT_DATA_DIR+'/1987.h5')\n",
    "df = store['df']\n",
    "labels_all_sorted = store['labels_all_sorted']\n",
    "store.close()\n",
    "\n",
    "print(len(df), 'articles')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual data\n",
    "\n",
    "There are two potential columns to be used as the input text source: `full_text` and `lead_paragraph`. The former is the full text of the article, whereas the latter is a shorter abstract of the contents of the article. \n",
    "\n",
    "Let's inspect the distributions of the lengths of these columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_lead, len_full = np.empty(len(df)), np.empty(len(df))\n",
    "for i, row in df.iterrows():\n",
    "   len_lead[i]=len(row['lead_paragraph'])\n",
    "   len_full[i]=len(row['full_text'])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title('Length of lead_paragraphs, mean: %.2f' % np.mean(len_lead))\n",
    "plt.xlabel('words')\n",
    "plt.hist(len_lead, 'auto')\n",
    "plt.subplot(122)\n",
    "plt.title('Length of full_text, mean: %.2f' % np.mean(len_full))\n",
    "plt.xlabel('words')\n",
    "plt.hist(len_full, 'auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the lead paragraphs and full texts of 5 random articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.sample(5)\n",
    "for i, row in dfs.iterrows():\n",
    "    print('='*80)\n",
    "    print (i, ': ', row['title'], '\\n\\n', row['lead_paragraph'], '\\n\\n', row['full_text'], sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we decide to use either the `transcipt` or the `description` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = \"full_text\"\n",
    "#texttype = \"lead_paragraph\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "\n",
    "We use the `NLABELS` most frequent descriptor tags as keyword labels we wish to predict. We'll encode the labels as `NLABELS`-dimensional binary vectors and store these in a new column `labels`.\n",
    "\n",
    "Then we take a look at the most common tags, and plot histograms of all tags and the used labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLABELS=100\n",
    "\n",
    "def indices_to_labels(x):\n",
    "    labels = np.zeros(NLABELS)\n",
    "    for i in x:\n",
    "        if i < NLABELS:\n",
    "            labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "df['labels'] = df['descriptor_indices'].apply(indices_to_labels)\n",
    "\n",
    "ntags = dict()\n",
    "l, ll = np.zeros(len(df)), np.zeros(len(df))\n",
    "\n",
    "for i, tl in df['descriptor'].iteritems():    \n",
    "    for t in tl:\n",
    "        if t == \"Terms not available\":\n",
    "            continue\n",
    "        l[i] += 1\n",
    "        if t in ntags:\n",
    "            ntags[t] += 1\n",
    "        else:\n",
    "            ntags[t] = 1\n",
    "\n",
    "for i, labv in df['labels'].iteritems():    \n",
    "    ll[i] = np.sum(labv)\n",
    "            \n",
    "nrows = len(df)\n",
    "print('Total of', len(labels_all_sorted), 'descriptor tags. Showing', NLABELS, 'most common tags:')\n",
    "for i, t in enumerate(labels_all_sorted[:NLABELS]):\n",
    "    print(i, t, ntags[t], \"%.4f\" % (ntags[t]/nrows))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('All descriptor tags and used labels, means: %.2f tags, %.2f labels' % (np.mean(l), np.mean(ll)))\n",
    "plt.xlabel('tags/labels per article')\n",
    "plt.hist([l,ll],np.arange(0,15), align='left', label=['descriptor tags', 'labels'])\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce input and label tensors\n",
    "\n",
    "We vectorize the text samples and labels into a 2D integer tensors. `MAX_NUM_WORDS` is the number of different words to use as tokens, selected based on word frequency. `MAX_SEQUENCE_LENGTH` is the fixed sequence length obtained by truncating or padding the original sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000 \n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts([x for x in df[texttype]])\n",
    "sequences = tokenizer.texts_to_sequences([x for x in df[texttype]])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.asarray([x for x in df['labels']])\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into a training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SET, TEST_SET = 10000, 10000\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, \n",
    "                                                    test_size=TEST_SET,\n",
    "                                                    shuffle=True, random_state=42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size=VALIDATION_SET,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "print('Shape of training data tensor:', x_train.shape)\n",
    "print('Shape of training label tensor:', y_train.shape)\n",
    "print('Shape of validation data tensor:', x_val.shape)\n",
    "print('Shape of validation label tensor:', y_val.shape)\n",
    "print('Shape of test data tensor:', x_test.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the embedding matrix by retrieving the corresponding word embedding for each token in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Shape of embedding matrix:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D CNN\n",
    "\n",
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(NLABELS, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "batch_size=64\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "To further analyze the results, we can produce the actual predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected threshold controls the number of label predictions we'll make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.25\n",
    "\n",
    "avg_n_gt, avg_n_pred = 0, 0\n",
    "for t in range(len(y_test)):\n",
    "    avg_n_gt += len(np.where(y_test[t]>0.5)[0])\n",
    "    avg_n_pred += len(np.where(predictions[t]>threshold)[0])\n",
    "avg_n_gt /= len(y_test)\n",
    "avg_n_pred /= len(y_test)\n",
    "print('Average number of ground-truth labels per talk: %.2f' % avg_n_gt)\n",
    "print('Average number of predicted labels per talk: %.2f' % avg_n_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correct and predicted labels for some talks in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_articles_to_show = 20\n",
    "\n",
    "for t in range(nb_articles_to_show):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_test[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+labels_all_sorted[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+labels_all_sorted[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has some applicable performance [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) we can try: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.precision_score(y_test.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('Recall: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.recall_score(y_test.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('F1 score: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.f1_score(y_test.flatten(), predictions.flatten()>threshold), threshold))\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_test.flatten(), predictions.flatten())\n",
    "print('Average precision: {0:.3f}'.format(average_precision))\n",
    "print('Coverage: {0:.3f}'\n",
    "      .format(metrics.coverage_error(y_test, predictions)))\n",
    "print('LRAP: {0:.3f}'\n",
    "      .format(metrics.label_ranking_average_precision_score(y_test, predictions)))\n",
    "\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_test.flatten(), predictions.flatten())\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-recall curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(CuDNNLSTM(256, return_sequences=True))\n",
    "model.add(CuDNNLSTM(256))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(NLABELS, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "batch_size=64\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "avg_n_gt, avg_n_pred = 0, 0\n",
    "for t in range(len(y_test)):\n",
    "    avg_n_gt += len(np.where(y_test[t]>0.5)[0])\n",
    "    avg_n_pred += len(np.where(predictions[t]>threshold)[0])\n",
    "avg_n_gt /= len(y_test)\n",
    "avg_n_pred /= len(y_test)\n",
    "print('Average number of ground-truth labels per talk: %.2f' % avg_n_gt)\n",
    "print('Average number of predicted labels per talk: %.2f' % avg_n_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_articles_to_show = 20\n",
    "\n",
    "for t in range(nb_articles_to_show):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_test[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+labels_all_sorted[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+labels_all_sorted[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.precision_score(y_test.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('Recall: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.recall_score(y_test.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('F1 score: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.f1_score(y_test.flatten(), predictions.flatten()>threshold), threshold))\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_test.flatten(), predictions.flatten())\n",
    "print('Average precision: {0:.3f}'.format(average_precision))\n",
    "print('Coverage: {0:.3f}'\n",
    "      .format(metrics.coverage_error(y_test, predictions)))\n",
    "print('LRAP: {0:.3f}'\n",
    "      .format(metrics.label_ranking_average_precision_score(y_test, predictions)))\n",
    "\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_test.flatten(), predictions.flatten())\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-recall curve');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

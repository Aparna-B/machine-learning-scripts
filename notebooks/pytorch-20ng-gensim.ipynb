{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups text classification with pre-trained word embeddings\n",
    "\n",
    "In this notebook, we'll use pre-trained [GloVe word embeddings](http://nlp.stanford.edu/projects/glove/) for text classification using PyTorch. Tokenization and word-to-id mapping is done using [gensim](https://radimrehurek.com/gensim/index.html). This notebook is largely based on the blog post [Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) by FranÃ§ois Chollet.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import __version__ as gensim_version \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    devicename = \"\"\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__,\n",
    "      'gensim version:', gensim_version,\n",
    "      'Device:', device, devicename)\n",
    "assert(LV(torch.__version__) >= LV(\"1.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a tool for visualizing progress during training.  Although TensorBoard was created for TensorFlow, it can also be used with PyTorch.  It is easiest to use it with the tensorboardX module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorboardX\n",
    "    import os, datetime\n",
    "    logdir = os.path.join(os.getcwd(), \"logs\",\n",
    "                          \"20ng-\"+datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    print('TensorBoard log directory:', logdir)\n",
    "    os.makedirs(logdir)\n",
    "    log = tensorboardX.SummaryWriter(logdir)\n",
    "except ImportError as e:\n",
    "    log = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word embeddings\n",
    "\n",
    "Let's begin by loading a datafile containing pre-trained word embeddings from [Pouta Object Storage](https://research.csc.fi/pouta-object-storage).  The datafile contains 100-dimensional embeddings for 400,000 English words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://object.pouta.csc.fi/swift/v1/AUTH_dac/mldata/glove6b100dtxt.zip\n",
    "!unzip -n glove6b100dtxt.zip\n",
    "GLOVE_DIR = \".\"\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print('Examples of embeddings:')\n",
    "for w in ['some', 'random', 'words']:\n",
    "    print(w, embeddings_index[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroups data set\n",
    "\n",
    "Next we'll load the [20 Newsgroups](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) data set. \n",
    "\n",
    "The dataset contains 20000 messages collected from 20 different Usenet newsgroups (1000 messages from each group):\n",
    "\n",
    "|[]()|[]()|[]()|[]()|\n",
    "| --- | --- |--- | --- |\n",
    "| alt.atheism           | soc.religion.christian   | comp.windows.x     | sci.crypt |               \n",
    "| talk.politics.guns    | comp.sys.ibm.pc.hardware | rec.autos          | sci.electronics |              \n",
    "| talk.politics.mideast | comp.graphics            | rec.motorcycles    | sci.space |                   \n",
    "| talk.politics.misc    | comp.os.ms-windows.misc  | rec.sport.baseball | sci.med |                     \n",
    "| talk.religion.misc    | comp.sys.mac.hardware    | rec.sport.hockey   | misc.forsale |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://object.pouta.csc.fi/swift/v1/AUTH_dac/mldata/news20.tar.gz\n",
    "!tar -x --skip-old-files -f news20.tar.gz\n",
    "TEXT_DATA_DIR = \"./20_newsgroup\"\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First message and its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0])\n",
    "print('label:', labels[0], labels_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the texts using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list()\n",
    "for text in texts:\n",
    "    tokens.append(simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the text samples into a 2D integer tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000 # 2 words reserved: 0=pad, 1=oov\n",
    "MAX_SEQUENCE_LENGTH = 1000 \n",
    "\n",
    "dictionary = Dictionary(tokens)\n",
    "dictionary.filter_extremes(no_below=0, no_above=1.0,\n",
    "                           keep_n=MAX_NUM_WORDS-2)\n",
    "\n",
    "word_index = dictionary.token2id\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = [dictionary.doc2idx(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate and pad sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i[:MAX_SEQUENCE_LENGTH] for i in data]\n",
    "data = np.array([np.pad(i, (0, MAX_SEQUENCE_LENGTH-len(i)), \n",
    "                        mode='constant', constant_values=-2)\n",
    "                 for i in data], dtype=int)\n",
    "data = data + 2\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Length of label vector:', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training set and a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SET, TEST_SET = 1000, 4000\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, \n",
    "                                                    test_size=TEST_SET,\n",
    "                                                    shuffle=True, random_state=42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size=VALIDATION_SET,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "print('Shape of training data tensor:', x_train.shape)\n",
    "print('Length of training label vector:', len(y_train))\n",
    "print('Shape of validation data tensor:', x_val.shape)\n",
    "print('Length of validation label vector:', len(y_val))\n",
    "print('Shape of test data tensor:', x_test.shape)\n",
    "print('Length of test label vector:', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PyTorch *DataLoader*s for all data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_dataset = TensorDataset(torch.LongTensor(x_train),\n",
    "                              torch.LongTensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=4)\n",
    "print(len(train_dataset), 'messages')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_dataset = TensorDataset(torch.LongTensor(x_val),\n",
    "                                   torch.LongTensor(y_val))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE,\n",
    "                               shuffle=False, num_workers=4)\n",
    "print(len(validation_dataset), 'messages')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_dataset = TensorDataset(torch.LongTensor(x_test),\n",
    "                             torch.LongTensor(y_test))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=4)\n",
    "print(len(test_dataset), 'messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "n_not_found = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS-2:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i+2] = embedding_vector\n",
    "    else:\n",
    "        n_not_found += 1\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)\n",
    "print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "print('Words not found in pre-trained embeddings:', n_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D CNN\n",
    "\n",
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1 = nn.Conv1d(100, 128, 5)\n",
    "        self.pool1 = nn.MaxPool1d(5)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 5)\n",
    "        self.pool2 = nn.MaxPool1d(5)\n",
    "        self.conv3 = nn.Conv1d(128, 128, 5)\n",
    "        self.pool3 = nn.MaxPool1d(35)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "    \n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, loss_vector=None, accuracy_vector=None):\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    pred_vector = torch.LongTensor()\n",
    "    pred_vector = pred_vector.to(device)\n",
    "    \n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss += criterion(output, target).data.item()\n",
    "\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        pred_vector = torch.cat((pred_vector, pred))\n",
    "\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    loss /= len(validation_loader)\n",
    "    if loss_vector is not None:\n",
    "        loss_vector.append(loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(loader.dataset)\n",
    "    if accuracy_vector is not None:\n",
    "        accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, correct, len(loader.dataset), accuracy))\n",
    "\n",
    "    return np.array(pred_vector.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    with torch.no_grad():\n",
    "        print('\\nValidation set:')\n",
    "        evaluate(validation_loader, lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), lossv)\n",
    "plt.title('validation loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), accv)\n",
    "plt.title('validation accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We evaluate the model using the test set. If accuracy on the test set is notably worse than with the training set, the model has likely overfitted to the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    predictions = evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at classification accuracies separately for each newsgroup, and compute a confusion matrix to see which newsgroups get mixed the most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test, predictions, labels=list(range(20)))\n",
    "\n",
    "print('Classification accuracy for each newsgroup:'); print()\n",
    "labels = [l[0] for l in sorted(labels_index.items(), key=lambda x: x[1])]\n",
    "for i,j in enumerate(cm.diagonal()/cm.sum(axis=1)): print(\"%s: %.4f\" % (labels[i].ljust(26), j))\n",
    "print()\n",
    "\n",
    "print('Confusion matrix (rows: true newsgroup; columns: predicted newsgroup):'); print()\n",
    "np.set_printoptions(linewidth=9999)\n",
    "print(cm); print()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cm, cmap=\"gray\", interpolation=\"none\")\n",
    "plt.grid(False)\n",
    "plt.title('Confusion matrix (rows: true newsgroup; columns: predicted newsgroup)')\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=90)\n",
    "plt.yticks(tick_marks, labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(100, 128, num_layers=2, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = h_n[1,:,:]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    with torch.no_grad():\n",
    "        print('\\nValidation set:')\n",
    "        evaluate(validation_loader, lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), lossv)\n",
    "plt.title('validation loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), accv)\n",
    "plt.title('validation accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    predictions = evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test, predictions, labels=list(range(20)))\n",
    "\n",
    "print('Classification accuracy for each newsgroup:'); print()\n",
    "labels = [l[0] for l in sorted(labels_index.items(), key=lambda x: x[1])]\n",
    "for i,j in enumerate(cm.diagonal()/cm.sum(axis=1)): print(\"%s: %.4f\" % (labels[i].ljust(26), j))\n",
    "print()\n",
    "\n",
    "print('Confusion matrix (rows: true newsgroup; columns: predicted newsgroup):'); print()\n",
    "np.set_printoptions(linewidth=9999)\n",
    "print(cm); print()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cm, cmap=\"gray\", interpolation=\"none\")\n",
    "plt.grid(False)\n",
    "plt.title('Confusion matrix (rows: true newsgroup; columns: predicted newsgroup)')\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=90)\n",
    "plt.yticks(tick_marks, labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talks keyword labeling with pre-trained word embeddings\n",
    "\n",
    "In this notebook, we'll use pre-trained [GloVe word embeddings](http://nlp.stanford.edu/projects/glove/) for keyword labeling using PyTorch. This notebook is largely based on the blog post [Using pre-trained word embeddings in a Keras model](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) by FranÃ§ois Chollet.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "assert(LV(torch.__version__) >= LV(\"1.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a tool for visualizing progress during training.  Although TensorBoard was created for TensorFlow, it can also be used with PyTorch.  It is easiest to use it with the tensorboardX module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorboardX\n",
    "    import os, datetime\n",
    "    logdir = os.path.join(os.getcwd(), \"logs\",\n",
    "                          \"ted-\"+datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    print('TensorBoard log directory:', logdir)\n",
    "    os.makedirs(logdir)\n",
    "    log = tensorboardX.SummaryWriter(logdir)\n",
    "except ImportError as e:\n",
    "    log = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word embeddings\n",
    "\n",
    "Let's begin by loading a datafile containing pre-trained word embeddings from [Pouta Object Storage](https://research.csc.fi/pouta-object-storage).  The datafile contains 100-dimensional embeddings for 400,000 English words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://object.pouta.csc.fi/swift/v1/AUTH_dac/mldata/glove6b100dtxt.zip\n",
    "!unzip -u glove6b100dtxt.zip\n",
    "GLOVE_DIR = \".\"\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = len(coefs)\n",
    "print('Found %d word vectors of dimensionality %d.' % (len(embeddings_index), embedding_dim))\n",
    "\n",
    "print('Examples of embeddings:')\n",
    "for w in ['some', 'random', 'words']:\n",
    "    print(w, embeddings_index[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TED Talks data set\n",
    "\n",
    "Next we'll load the TED Talks data set (Kaggle [TED Talks](https://www.kaggle.com/rounakbanik/ted-talks), 2017 edition).  The data is stored in two CSV files, so we load both of them and merge them into a single DataFrame. \n",
    "\n",
    "The merged dataset contains transcripts and metadata of 2467 TED talks. Each talk is also annotated with a set of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://object.pouta.csc.fi/swift/v1/AUTH_dac/mldata/ted-talks.zip\n",
    "!unzip -u ted-talks.zip\n",
    "TEXT_DATA_DIR = \".\"\n",
    "\n",
    "df1 = pd.read_csv(TEXT_DATA_DIR+'/ted_main.csv')\n",
    "df2 = pd.read_csv(TEXT_DATA_DIR+'/transcripts.csv')\n",
    "df = pd.merge(left=df1, right=df2, how='inner', left_on='url', right_on='url')\n",
    "\n",
    "print(len(df), 'talks')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual data\n",
    "\n",
    "There are two potential columns to be used as the input text source: `transcript` and `description`. The former is the full transcript of the talk, whereas the latter is a shorter abstract of the contents of the talk. \n",
    "\n",
    "Let's inspect the distributions of the lengths of these columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_trans, len_descr = np.empty(len(df)), np.empty(len(df))\n",
    "for i, row in df.iterrows():\n",
    "   len_trans[i]=len(row['transcript'])\n",
    "   len_descr[i]=len(row['description'])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title('Length of descriptions, mean: %.2f' % np.mean(len_descr))\n",
    "plt.xlabel('words')\n",
    "plt.hist(len_descr, 'auto')\n",
    "plt.subplot(122)\n",
    "plt.title('Length of transcripts, mean: %.2f' % np.mean(len_trans))\n",
    "plt.xlabel('words')\n",
    "plt.hist(len_trans, 'auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we decide to use either the `transcipt` or the `description` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = \"transcript\"\n",
    "#texttype = \"description\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "\n",
    "Let's start by converting the string-type lists of tags to Python lists.  Then, we take a look at a histogram of number of tags attached to talks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['taglist']=df['tags'].apply(lambda x: ast.literal_eval(x))\n",
    "df.head()\n",
    "\n",
    "l = np.empty(len(df))\n",
    "for i, v in df['taglist'].iteritems():\n",
    "    l[i]=len(v)\n",
    "plt.figure()\n",
    "plt.title('Number of tags, mean: %.2f' % np.mean(l))\n",
    "plt.xlabel('labels')\n",
    "plt.hist(l,np.arange(40)+1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `NLABELS` most frequent tags as keyword labels we wish to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLABELS=100\n",
    "\n",
    "ntags = dict()\n",
    "for tl in df['taglist']:\n",
    "    for t in tl:\n",
    "        if t in ntags:\n",
    "            ntags[t] += 1\n",
    "        else:\n",
    "            ntags[t] = 1\n",
    "\n",
    "ntagslist_sorted = sorted(ntags, key=ntags.get, reverse=True)\n",
    "print('Total of', len(ntagslist_sorted), 'tags found. Showing', NLABELS, 'most common tags:')\n",
    "for i, t in enumerate(ntagslist_sorted[:NLABELS]):\n",
    "    print(i, t, ntags[t])\n",
    "\n",
    "def tags_to_indices(x):\n",
    "    ilist = []\n",
    "    for t in x:\n",
    "        ilist.append(ntagslist_sorted.index(t))\n",
    "    return ilist\n",
    "\n",
    "df['tagidxlist'] = df['taglist'].apply(tags_to_indices)\n",
    "\n",
    "def indices_to_labels(x):\n",
    "    labels = np.zeros(NLABELS)\n",
    "    for i in x:\n",
    "        if i < NLABELS:\n",
    "            labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "df['labels'] = df['tagidxlist'].apply(indices_to_labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce input and label tensors\n",
    "\n",
    "We vectorize the text samples and labels into a 2D integer tensors. `MAX_NUM_WORDS` is the number of different words to use as tokens, selected based on word frequency. `MAX_SEQUENCE_LENGTH` is the fixed sequence length obtained by truncating or padding the original sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000 \n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts([x for x in df[texttype]])\n",
    "sequences = tokenizer.texts_to_sequences([x for x in df[texttype]])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.asarray([x for x in df['labels']])\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into a training set and a validation set.  We use a fraction of the data specified by `VALIDATION_DATA` for validation.  Note that we do not use a separate test set in this notebook, due to the small size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "print('Shape of training data tensor:', x_train.shape)\n",
    "print('Shape of training label tensor:', y_train.shape)\n",
    "print('Shape of validation data tensor:', x_val.shape)\n",
    "print('Shape of validation label tensor:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_dataset = TensorDataset(torch.LongTensor(x_train),\n",
    "                              torch.FloatTensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=4)\n",
    "print(len(train_dataset), 'talks')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_dataset = TensorDataset(torch.LongTensor(x_val),\n",
    "                                   torch.FloatTensor(y_val))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE,\n",
    "                               shuffle=False, num_workers=4)\n",
    "print(len(validation_dataset), 'talks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the embedding matrix by retrieving the corresponding word embedding for each token in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)\n",
    "print('Shape of embedding matrix:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D CNN\n",
    "\n",
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.conv1 = nn.Conv1d(100, 128, 5)\n",
    "        self.pool1 = nn.MaxPool1d(5)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 5)\n",
    "        self.pool2 = nn.MaxPool1d(5)\n",
    "        self.conv3 = nn.Conv1d(128, 128, 5)\n",
    "        self.pool3 = nn.MaxPool1d(35)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, NLABELS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x))\n",
    "        #return F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "    \n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, loss_vector=None):\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    pred_vector = torch.FloatTensor()\n",
    "    pred_vector = pred_vector.to(device)\n",
    "    \n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss += criterion(output, target).data.item()\n",
    "\n",
    "        pred = output.data\n",
    "        pred_vector = torch.cat((pred_vector, pred))\n",
    "\n",
    "    loss /= len(validation_loader)\n",
    "    if loss_vector is not None:\n",
    "        loss_vector.append(loss)\n",
    "    \n",
    "    print('Average loss: {:.4f}\\n'.format(loss))\n",
    "\n",
    "    return np.array(pred_vector.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "lossv = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    with torch.no_grad():\n",
    "        print('\\nValidation set:')\n",
    "        evaluate(validation_loader, lossv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), lossv)\n",
    "plt.title('validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "To further analyze the results, we can produce the actual predictions for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    predictions = evaluate(validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected threshold controls the number of label predictions we'll make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "avg_n_gt, avg_n_pred = 0, 0\n",
    "for t in range(len(y_val)):\n",
    "    avg_n_gt += len(np.where(y_val[t]>0.5)[0])\n",
    "    avg_n_pred += len(np.where(predictions[t]>threshold)[0])\n",
    "avg_n_gt /= len(y_val)\n",
    "avg_n_pred /= len(y_val)\n",
    "print('Average number of ground-truth labels per talk: %.2f' % avg_n_gt)\n",
    "print('Average number of predicted labels per talk: %.2f' % avg_n_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correct and predicted labels for some talks in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_talks_to_show = 20\n",
    "\n",
    "for t in range(nb_talks_to_show):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_val[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, the F1 measure, and NDCG (normalized discounted cumulative gain) after *k* returned labels are common performance metrics for multi-label classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(vals, k):\n",
    "    res = 0\n",
    "    for i in range(k):\n",
    "        res += vals[i][1] / np.log2(i + 2)\n",
    "    return res\n",
    "\n",
    "def scores_at_k(truevals, predvals, k):\n",
    "    precision_at_k, recall_at_k, f1score_at_k, ndcg_at_k = 0, 0, 0, 0\n",
    "\n",
    "    for j in range(len(truevals)): \n",
    "        z = list(zip(predvals[j], truevals[j]))\n",
    "        sorted_z = sorted(z, reverse=True, key=lambda tup: tup[0])\n",
    "        opt_z = sorted(z, reverse=True, key=lambda tup: tup[1])\n",
    "        truesum = 0\n",
    "        for i in range(k):\n",
    "            truesum += sorted_z[i][1]\n",
    "        pr = truesum / k\n",
    "        rc = truesum / np.sum(truevals[0])\n",
    "        if truesum>0:\n",
    "            f1score_at_k += 2*((pr*rc)/(pr+rc))\n",
    "        precision_at_k += pr\n",
    "        recall_at_k += rc\n",
    "        cg = dcg_at_k(sorted_z, k) / (dcg_at_k(opt_z, k) + 0.00000001)\n",
    "        ndcg_at_k += cg\n",
    "\n",
    "    precision_at_k /= len(truevals)\n",
    "    recall_at_k /= len(truevals)\n",
    "    f1score_at_k /= len(truevals)\n",
    "    ndcg_at_k /= len(truevals)\n",
    "    \n",
    "    print('Precision@{0} : {1:.2f}'.format(k, precision_at_k))\n",
    "    print('Recall@{0}    : {1:.2f}'.format(k, recall_at_k))\n",
    "    print('F1@{0}        : {1:.2f}'.format(k, f1score_at_k))\n",
    "    print('NDCG@{0}      : {1:.2f}'.format(k, ndcg_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_at_k(y_val, predictions, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has also some applicable performance [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) we can try: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.precision_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('Recall: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.recall_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('F1 score: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.f1_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_val.flatten(), predictions.flatten())\n",
    "print('Average precision: {0:.3f}'.format(average_precision))\n",
    "print('Coverage: {0:.3f}'\n",
    "      .format(metrics.coverage_error(y_val, predictions)))\n",
    "print('LRAP: {0:.3f}'\n",
    "      .format(metrics.label_ranking_average_precision_score(y_val, predictions)))\n",
    "\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_val.flatten(), predictions.flatten())\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-recall curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(100, 128, num_layers=2, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, NLABELS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = h_n[1,:,:]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x))\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "lossv = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    with torch.no_grad():\n",
    "        print('\\nValidation set:')\n",
    "        evaluate(validation_loader, lossv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), lossv)\n",
    "plt.title('validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    predictions = evaluate(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "avg_n_gt, avg_n_pred = 0, 0\n",
    "for t in range(len(y_val)):\n",
    "    avg_n_gt += len(np.where(y_val[t]>0.5)[0])\n",
    "    avg_n_pred += len(np.where(predictions[t]>threshold)[0])\n",
    "avg_n_gt /= len(y_val)\n",
    "avg_n_pred /= len(y_val)\n",
    "print('Average number of ground-truth labels per talk: %.2f' % avg_n_gt)\n",
    "print('Average number of predicted labels per talk: %.2f' % avg_n_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_talks_to_show = 20\n",
    "\n",
    "for t in range(nb_talks_to_show):\n",
    "    print(t,':')\n",
    "    print('    correct: ', end='')\n",
    "    for idx in np.where(y_val[t]>0.5)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()\n",
    "    print('  predicted: ', end='')\n",
    "    for idx in np.where(predictions[t]>threshold)[0].tolist():\n",
    "        sys.stdout.write('['+ntagslist_sorted[idx]+'] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_at_k(y_val, predictions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.precision_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('Recall: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.recall_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "print('F1 score: {0:.3f} (threshold: {1:.2f})'\n",
    "      .format(metrics.f1_score(y_val.flatten(), predictions.flatten()>threshold), threshold))\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_val.flatten(), predictions.flatten())\n",
    "print('Average precision: {0:.3f}'.format(average_precision))\n",
    "print('Coverage: {0:.3f}'\n",
    "      .format(metrics.coverage_error(y_val, predictions)))\n",
    "print('LRAP: {0:.3f}'\n",
    "      .format(metrics.label_ranking_average_precision_score(y_val, predictions)))\n",
    "\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_val.flatten(), predictions.flatten())\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-recall curve');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
